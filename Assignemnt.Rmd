---
title: "Human Activity Recognition Data - Weight Lifting Exercises"
author: "thebloodymummer"
date: "28 May 2018"
output: html_document
---

##Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The training data was acquired from: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]

The test data was acquired from: 
[https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv]

##Data and Method
For this study, six young and healthy participents were asked to perform a single set of 10 repititions of Unilateral Dumbbell Curl. This is distinguished in the data set by 5 different classes: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). 

Initially, the training data set will be cross validated by subsetting it into a new training set (60%) and a validation set (40%) without replacement. The models will be fitted using the new training data set and initially tested on the validation set. The best model will then be tested on the original test set supplied.

The optimum model will be chosen by applying Decision Tree and Random Forest methods to the training and validation datasets. 

##Relevant packages, seeding and Data Acquisition

The following packages were loaded. The machine learning will be covered by caret, Decision trees and Random forests by rpart, randomForest and rattle while plots will be covered by ggplot. TAlso, to ensure reproducibility, an initial seed of 33 was chosen.

```{r, message=F, warning=F, echo = TRUE}
library(caret)
library(rpart)
library(randomForest)
library(rattle)
library(ggplot2)
set.seed(333)
```

Using the links provided, the relavant data sets were downloaded into a "rawData" folder and read as csv files into RStudio. 

```{r, echo = TRUE}
if(!file.exists("rawData")){dir.create("rawData")}
train <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
traindest <- "./rawData/train.csv"
testdest <- "./rawData/test.csv"
download.file(train, destfile = traindest)
download.file(test, destfile = testdest)
trainData <- read.csv("./rawData/train.csv", na.strings=c("NA","#DIV/0!", ""))
testData <- read.csv("./rawData/test.csv", na.strings=c("NA","#DIV/0!", ""))
```

The data was then tidied to remove unnecessary columns, missing values and near zero values.

```{r, echo = TRUE}
trainData <- trainData[,8:length(colnames(trainData))]
testData <- testData[,8:length(colnames(testData))]
trainData <- trainData[, colSums(is.na(trainData)) == 0] 
testData <- testData[, colSums(is.na(testData)) == 0]
nzv <- nearZeroVar(trainData,saveMetrics=TRUE)
zero.var.ind <- sum(nzv$nzv)
if ((zero.var.ind>0)) { trainData <- trainData[,nzv$nzv==FALSE]}
```

##Preproccessing
As mentioned earlier, the supplied training data is partitioned into a new training set called trainData2 (60%) and validation set called validation (40%). 

```{r, echo = TRUE}
inTrain <- createDataPartition(y=trainData$classe, p=0.6, list=FALSE)
trainData2 <- trainData[inTrain, ] 
validation <- trainData[-inTrain, ]
dim(trainData2); dim(validation)
```

To see an even spread of oberservations across the classe varible, a histogram was plotted demonstrating the frequency of each class.

```{r, echo = TRUE}
ggplot(trainData2, 
       aes(x = classe)) + geom_bar() + labs(main = "Plot of levels of variable classes",
                                            x = "Classe", 
                                            y = "Frequency")
```

##Classification Tree
A model is established and the resulting classification tree and confusion matrix is printed.

```{r, echo = TRUE}
classModel <- rpart(classe ~ ., data=trainData2, method="class")
classPredication <- predict(classModel, validation, type = "class")
fancyRpartPlot(classModel)
```

```{r, echo = TRUE}
confusionMatrix(classPredication, validation$classe)
```

##Random Forest
Similarly, a model is generated using a random forest and the confusion matrix is printed.

```{r, echo = TRUE}
forestModel <- randomForest(classe ~. , data=trainData2, method="class")
forestPrediction <- predict(forestModel, validation, type = "class")
confusionMatrix(forestPrediction, validation$classe)
```

##Comparison between the two models

Fomr the data shown above, it seems the Random Forest performed better than the Decision Tree. The accuracy for the Decision Tree was 0.739 while the Robdom Forest was 0.993. Simarly, the Kappa statistic for the Decision Tree demonstrated a value of 0.668 and a value of 0.992 for the Random Forest. As a result the Random Forest model is chosen due to its out-of-sample error of 0.007. Therefore very few of the test sample values will be misclassified.

##Prediction on Test Set

The Random Forest model will be applied to test which classe is predicted based on the other varibles with 99.3% accuracy.
```{r, echo = TRUE}
predict(forestModel, testData, type="class")
```

##References

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 

